{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yGcfZrnm8W4Y"
   },
   "source": [
    "# Тестовое задание\n",
    "<br>\n",
    "\n",
    "### Общее описание\n",
    "В этом задании Вам необходимо реализовать и обучить алгоритмы DDPG и TD3, а затем сравнить резульаты, полученные данными алгритмами. Тестирование проводится с помощью одной из классических сред LunarLanderContinuous, которая входит в библиотеку OpenAI Gym.<br><br>\n",
    "При решении задачи крайне приветствуется внесение в алгоритмы спецефичных для данной задачи модификаций.\n",
    "\n",
    "### Критерии оценки\n",
    "\n",
    "* Задача полностью решена, если агент в среднем достигает 220+ очков\n",
    "* Задача решена удовлетворительно, если агент в среднем достигает 50+ очков\n",
    "* Помимо результатов обучения агента, также оценивается качество кода и анализа результатов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "7wXp1J6K8W4c"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Устройство, на котором будет работать PyTorch.\n",
    "device = torch.device('cuda') # Model trains faster in GPU\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "def save_agent(agent, logger):\n",
    "    if isinstance(agent, TD3):\n",
    "        save_paths = [\n",
    "            f\"saved_models/td3_best_weights_actor.pt\",\n",
    "            f\"saved_models/td3_best_weights_critic.pt\",\n",
    "            f\"saved_models/td3_best_weights_critic2.pt\",\n",
    "            f\"saved_models/td3_best_weights_target_actor.pt\",\n",
    "            f\"saved_models/td3_best_weights_target_critic.pt\",\n",
    "            f\"saved_models/td3_best_weights_target_critic2.pt\",\n",
    "        ]\n",
    "        save_nets = [\n",
    "            agent.actor, agent.critic, agent.critic2, agent.target_actor,\n",
    "            agent.target_critic, agent.target_critic2,\n",
    "        ]\n",
    "\n",
    "    elif isinstance(agent, DDPG):\n",
    "        save_paths = [\n",
    "            f\"saved_models/ddpg_best_weights_actor.pt\",\n",
    "            f\"saved_models/ddpg_best_weights_critic.pt\",\n",
    "            f\"saved_models/ddpg_best_weights_target_actor.pt\",\n",
    "            f\"saved_models/ddpg_best_weights_target_critic.pt\",\n",
    "        ]\n",
    "        save_nets = [agent.actor, agent.critic, agent.target_actor, agent.target_critic]\n",
    "\n",
    "    else:\n",
    "        save_paths = []\n",
    "        save_nets = []\n",
    "\n",
    "    for path, net in zip(save_paths, save_nets):\n",
    "        torch.save(net.state_dict(), path)\n",
    "        # logger.save(glob_str=path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "#@title Буфер опыта\n",
    "# Тут ничего менять не нужно\n",
    "\n",
    "class ExperienceReplay:\n",
    "    def __init__(self, size=10000):\n",
    "        self.data = []\n",
    "        self.max_size = size\n",
    "        self.i = 0\n",
    "\n",
    "    def add(self, transition):\n",
    "        if len(self.data) < self.max_size:\n",
    "            self.data.append(transition)\n",
    "        else:\n",
    "            self.data[self.i] = transition\n",
    "            self.i = (self.i + 1) % self.max_size\n",
    "\n",
    "    def sample(self, size):\n",
    "        batch = random.sample(self.data, size)\n",
    "        return list(zip(*batch))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "#@title Метод build_plot для отрисовки графиков\n",
    "# Тут ничего менять не нужно\n",
    "\n",
    "def build_plot(xs, means, stds, labels, figsize=(12, 8)):\n",
    "    colors = [\n",
    "        (0.8, 0.0, 0.0),\n",
    "        (0.0, 0.8, 0.0),\n",
    "        (0.0, 0.0, 0.8),\n",
    "        (0.0, 0.5, 0.8)\n",
    "    ]\n",
    "    x_min = max([x[0] for x in xs])\n",
    "    x_max = min([x[-1] for x in xs])\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.plot([x_min, x_max], [150., 150.], color=(0.25, 0.25, 0.25, 0.6), linestyle='-.', label='Minimal solution')\n",
    "    plt.plot([x_min, x_max], [300., 300.], color=(0.75, 0.75, 0.0, 0.6), linestyle='-.', label='Optimal solution')\n",
    "    for x, mean, std, label, color in zip(xs, means, stds, labels, colors):\n",
    "        plt.fill_between(x, mean - 2*std, mean + 2*std, color=color, alpha=0.2)\n",
    "        plt.plot(x, mean, label=label, color=color)\n",
    "    plt.legend()\n",
    "    plt.xlim(x_min, x_max)\n",
    "    plt.ylim(-300, 300)\n",
    "    plt.grid()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "#@title Методы train и test\n",
    "# Тут ничего менять не нужно\n",
    "\n",
    "ACTION_SIZE = 2\n",
    "STATE_SIZE = 8\n",
    "ENV_NAME = 'LunarLander-v2'\n",
    "\n",
    "def test(agent, episodes=10):\n",
    "    rewards = []\n",
    "    env = gym.make(ENV_NAME, continuous=True)\n",
    "    for _ in range(episodes):\n",
    "        sum_reward = 0\n",
    "        done, trunc = False, False\n",
    "        state, _ = env.reset()\n",
    "        while not (done or trunc):\n",
    "            state, reward, done, trunc, _ = env.step(agent.get_action(state))\n",
    "            sum_reward += reward\n",
    "        rewards.append(sum_reward)\n",
    "    return np.mean(rewards), np.std(rewards), agent.actor_scheduler.get_last_lr()\n",
    "\n",
    "def train(agent, timesteps=50000, batch_size=256, buffer_size=50000,\n",
    "          start_train=10000, test_every=1000, test_count=10, logger=None, action_noise=0.15):\n",
    "    env = gym.make(ENV_NAME, continuous=True)\n",
    "\n",
    "    torch.manual_seed(0)\n",
    "    random.seed(0)\n",
    "\n",
    "    buffer = ExperienceReplay(buffer_size)\n",
    "\n",
    "    actor_loss_sum = 0\n",
    "    critic_loss_sum = 0\n",
    "    loss_ctn = 0\n",
    "\n",
    "    max_mean_minus_std = 0\n",
    "\n",
    "    log_ts = []\n",
    "    log_mean = []\n",
    "    log_std = []\n",
    "\n",
    "    done, trunc = False, False\n",
    "    state, _ = env.reset()\n",
    "\n",
    "    rng = tqdm(range(timesteps))\n",
    "    for t in rng:\n",
    "        if done or trunc:\n",
    "            done, trunc = False, False\n",
    "            state, _ = env.reset()\n",
    "\n",
    "        state = torch.FloatTensor(state).to(device)\n",
    "\n",
    "        noise = np.random.randn(ACTION_SIZE) * action_noise * (1 - t / timesteps)\n",
    "        action = np.clip(agent.get_action(state).cpu().detach().numpy() + noise, -1, 1)\n",
    "\n",
    "        next_state, reward, done, trunc, _ = env.step(action)\n",
    "        buffer.add((state, action, next_state, reward, done, trunc))\n",
    "        state = next_state\n",
    "\n",
    "        if t > start_train:\n",
    "            batch = buffer.sample(batch_size)\n",
    "\n",
    "            critic_loss, actor_loss = agent.update(batch, t)\n",
    "            actor_loss_sum += actor_loss\n",
    "            critic_loss_sum += critic_loss\n",
    "            loss_ctn += 1\n",
    "\n",
    "            if t % test_every == 0 or t == timesteps - 1:\n",
    "                log_ts.append(t)\n",
    "                mean, std, current_lr = test(agent, test_count)\n",
    "                if logger is not None:\n",
    "                    logger.log({\n",
    "                        'mean': mean, 'std': std, 'step': t, 'cur_lr': current_lr,\n",
    "                        'critic_loss': critic_loss_sum / loss_ctn, 'actor_loss': actor_loss_sum / loss_ctn,\n",
    "                    }, step=t)\n",
    "                    if mean - std > max_mean_minus_std:\n",
    "                        save_agent(agent, logger)\n",
    "                log_mean.append(mean)\n",
    "                log_std.append(std)\n",
    "                rng.set_postfix_str(f'Mean reward: {mean:.2f}, Critic loss: {(critic_loss_sum / loss_ctn):.2f}, Actor loss: {(actor_loss_sum / loss_ctn):.2f}')\n",
    "                actor_loss_sum = 0\n",
    "                critic_loss_sum = 0\n",
    "                loss_ctn = 0\n",
    "\n",
    "    return np.array(log_ts), np.array(log_mean), np.array(log_std)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Задание\n",
    "Вам нужно заполнить пробелы в коде ниже. Код взаимодействия со средой уже реализован за Вас."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_size: int, action_size: int, hidden_size: int):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_size, hidden_size),\n",
    "            nn.Mish(inplace=True),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Mish(inplace=True),\n",
    "            nn.Linear(hidden_size, action_size),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, state: torch.FloatTensor):\n",
    "        \"\"\"\n",
    "        Принимает на вход state, который является объектом класса torch.Tensor и имеет размерность (batch_size, state_size)\n",
    "        Возвращает объект класса torch.Tensor размерности (batch_size, action_size), который является предсказанным действием агента\n",
    "        \"\"\"\n",
    "        return self.net(state)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_size: int, action_size: int, hidden_size: int):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_size + action_size, hidden_size),\n",
    "            nn.Mish(inplace=True),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Mish(inplace=True),\n",
    "            nn.Linear(hidden_size, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, state: torch.FloatTensor, action: torch.FloatTensor):\n",
    "        \"\"\"\n",
    "        Принимает на вход state, который является объектом класса torch.Tensor и имеет размерность (batch_size, state_size)\n",
    "        Принимает на вход action, который является объектом класса torch.Tensor и имеет размерность (batch_size, action_size)\n",
    "        Возвращает объект класса torch.Tensor размерности (batch_size, ), который является оценкой Q-function\n",
    "        \"\"\"\n",
    "        return torch.squeeze(self.net(torch.cat((state, action), 1)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "class DDPG:\n",
    "    def __init__(self, state_size, action_size, tau=0.001, gamma=0.99, lr=1e-4,\n",
    "                 adam_eps=1e-8, hidden_size=256, timesteps=1_000_000, max_grad_norm=0.5):\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "\n",
    "        self.actor = Actor(state_size, action_size, hidden_size)\n",
    "        self.critic = Critic(state_size, action_size, hidden_size)\n",
    "        self.target_actor = copy.deepcopy(self.actor)\n",
    "        self.target_critic = copy.deepcopy(self.critic)\n",
    "\n",
    "        self.actor.to(device)\n",
    "        self.target_actor.to(device)\n",
    "        self.critic.to(device)\n",
    "        self.target_critic.to(device)\n",
    "\n",
    "        self.actor_optim = optim.Adam(self.actor.parameters(), lr=lr, eps=adam_eps)\n",
    "        self.actor_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(self.actor_optim, timesteps)\n",
    "\n",
    "        self.critic_optim = optim.Adam(self.critic.parameters(), lr=lr, eps=adam_eps)\n",
    "        self.critic_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(self.critic_optim, timesteps)\n",
    "\n",
    "        self.critic_loss_fn = nn.MSELoss()\n",
    "\n",
    "    def get_action(self, state: torch.FloatTensor):\n",
    "        \"\"\"\n",
    "        Принимает на вход state, который является объектом класса torch.FloatTensor\n",
    "        Возвращает action, который является объектом класса torch.FloatTensor\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            action = self.actor(state)\n",
    "        return action\n",
    "\n",
    "    def compute_actor_loss(self, state):\n",
    "        \"\"\"\n",
    "        Принимает на вход state, который является объектом класса torch.Tensor и имеет размерность (batch_size, state_size)\n",
    "        Возвращает функцию потерь actor'а loss, которая явлеятся объектом класса torch.Tensor\n",
    "        \"\"\"\n",
    "        state = state.to(device)\n",
    "        action = self.actor(state)\n",
    "        loss = -self.critic(state, action)\n",
    "        return loss.mean()\n",
    "\n",
    "    def compute_critic_loss(self, state, action, next_state, reward, done, trunc):\n",
    "        \"\"\"\n",
    "        Принимает на вход state, который является объектом класса torch.Tensor и имеет размерность (batch_size, state_size)\n",
    "        Принимает на вход action, который является объектом класса torch.Tensor и имеет размерность (batch_size, action_size)\n",
    "        Принимает на вход next_state, который является объектом класса torch.Tensor и имеет размерность (batch_size, state_size)\n",
    "        Принимает на вход reward, который является объектом класса torch.Tensor и имеет размерность (batch_size, )\n",
    "        Принимает на вход done, который является объектом класса torch.Tensor и имеет размерность (batch_size, ) и тип bool\n",
    "        Возвращает функцию потерь critic'а loss, которая явлеятся объектом класса torch.Tensor\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            estimate: torch.Tensor = reward + self.gamma * self.target_critic(next_state, self.target_actor(next_state)) * (1 - (done & ~trunc).to(torch.float32))\n",
    "        loss = self.critic_loss_fn(estimate, self.critic(state, action))\n",
    "        return loss\n",
    "\n",
    "    def soft_update(self, target_net, source_net):\n",
    "        \"\"\"\n",
    "        Применяет soft update с коэффициентом self.tau обновляя параметры target_net с помощью параметров source_net\n",
    "        \"\"\"\n",
    "        for target_parameter, source_parameter in zip(target_net.parameters(), source_net.parameters()):\n",
    "            target_parameter.data.mul_(1 - self.tau)\n",
    "            target_parameter.data.add_(self.tau * source_parameter.data)\n",
    "\n",
    "    def update(self, batch, current_timestep):\n",
    "        state, action, next_state, reward, done, trunc = batch\n",
    "        state = torch.tensor(np.array(state), dtype=torch.float32, device=device)\n",
    "        next_state = torch.tensor(np.array(next_state), dtype=torch.float32, device=device)\n",
    "        reward = torch.tensor(reward, dtype=torch.float32, device=device).view(-1)\n",
    "        done = torch.tensor(done, device=device)\n",
    "        trunc = torch.tensor(trunc, device=device)\n",
    "        action = torch.tensor(np.array(action), device=device, dtype=torch.float32)\n",
    "\n",
    "        actor_loss = self.compute_actor_loss(state)\n",
    "        self.actor_optim.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        nn.utils.clip_grad_norm_([p for g in self.actor_optim.param_groups for p in g['params']], self.max_grad_norm)\n",
    "        self.actor_optim.step()\n",
    "        self.actor_scheduler.step(epoch=current_timestep)\n",
    "\n",
    "        # Actor soft update\n",
    "        self.soft_update(self.target_actor, self.actor)\n",
    "\n",
    "        critic_loss = self.compute_critic_loss(state, action, next_state, reward, done, trunc)\n",
    "        self.critic_optim.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        nn.utils.clip_grad_norm_([p for g in self.critic_optim.param_groups for p in g['params']], self.max_grad_norm)\n",
    "        self.critic_optim.step()\n",
    "        self.critic_scheduler.step(epoch=current_timestep)\n",
    "\n",
    "        # Critic soft update\n",
    "        self.soft_update(self.target_critic, self.critic)\n",
    "\n",
    "        return critic_loss.item(), actor_loss.item()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "class TD3(DDPG):\n",
    "    def __init__(self, state_size, action_size, tau=0.001, gamma=0.99, lr=1e-4, policy_freq=3,\n",
    "                 adam_eps=1e-8, hidden_size=256, timesteps=1_000_000, max_grad_norm=0.5):\n",
    "        super().__init__(state_size, action_size, tau=tau, gamma=gamma, lr=lr, adam_eps=adam_eps,\n",
    "                         hidden_size=hidden_size, timesteps=timesteps, max_grad_norm=max_grad_norm)\n",
    "        self.critic2 = Critic(state_size, action_size, hidden_size)\n",
    "        self.target_critic2 = copy.deepcopy(self.critic2)\n",
    "\n",
    "        self.critic2.to(device)\n",
    "        self.target_critic2.to(device)\n",
    "\n",
    "        self.critic2_optim = optim.Adam(self.critic2.parameters(), lr=lr, eps=adam_eps)\n",
    "        self.critic2_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(self.critic2_optim, timesteps)\n",
    "\n",
    "        self.policy_freq = policy_freq\n",
    "\n",
    "    def compute_critic_loss(self, state, action, next_state, reward, done, trunc):\n",
    "        \"\"\"\n",
    "        Принимает на вход state, который является объектом класса torch.Tensor и имеет размерность (batch_size, state_size)\n",
    "        Принимает на вход action, который является объектом класса torch.Tensor и имеет размерность (batch_size, action_size)\n",
    "        Принимает на вход next_state, который является объектом класса torch.Tensor и имеет размерность (batch_size, state_size)\n",
    "        Принимает на вход reward, который является объектом класса torch.Tensor и имеет размерность (batch_size, )\n",
    "        Принимает на вход done, который является объектом класса torch.Tensor и имеет размерность (batch_size, ) и тип bool\n",
    "        Возвращает функцию потерь critic'а loss, которая явлеятся объектом класса torch.Tensor\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            noise = (torch.randn(ACTION_SIZE, dtype=torch.float32, device=device) * 0.25).clamp(-0.3, 0.3)\n",
    "            noisy_action = (self.target_actor(next_state) + noise).clamp(-1, 1)\n",
    "            estimate = reward + self.gamma * torch.min(\n",
    "                self.target_critic(next_state, noisy_action),\n",
    "                self.target_critic2(next_state, noisy_action),\n",
    "            ).squeeze() * (1 - (done & ~trunc).to(torch.float32))\n",
    "\n",
    "        loss = self.critic_loss_fn(self.critic(state, action), estimate) \\\n",
    "               + self.critic_loss_fn(self.critic2(state, action), estimate)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def update(self, batch, current_timestep):\n",
    "        state, action, next_state, reward, done, trunc = batch\n",
    "\n",
    "        state = torch.tensor(np.array(state), dtype=torch.float32, device=device)\n",
    "        next_state = torch.tensor(np.array(next_state), dtype=torch.float32, device=device)\n",
    "        reward = torch.tensor(reward, dtype=torch.float32, device=device).view(-1)\n",
    "        done = torch.tensor(done, device=device)\n",
    "        trunc = torch.tensor(trunc, device=device)\n",
    "        action = torch.tensor(np.array(action), device=device, dtype=torch.float32)\n",
    "\n",
    "        actor_loss = torch.zeros(1)\n",
    "        critics_loss = self.compute_critic_loss(state, action, next_state, reward, done, trunc)\n",
    "\n",
    "        self.critic_optim.zero_grad()\n",
    "        self.critic2_optim.zero_grad()\n",
    "        critics_loss.backward()\n",
    "        nn.utils.clip_grad_norm_([p for g in self.critic_optim.param_groups for p in g['params']], self.max_grad_norm)\n",
    "        nn.utils.clip_grad_norm_([p for g in self.critic2_optim.param_groups for p in g['params']], self.max_grad_norm)\n",
    "        self.critic_optim.step()\n",
    "        self.critic2_optim.step()\n",
    "        self.critic_scheduler.step(epoch=current_timestep)\n",
    "        self.critic2_scheduler.step(epoch=current_timestep)\n",
    "\n",
    "        if current_timestep % self.policy_freq == 0:\n",
    "            actor_loss = self.compute_actor_loss(state)\n",
    "            self.actor_optim.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            nn.utils.clip_grad_norm_([p for g in self.actor_optim.param_groups for p in g['params']], self.max_grad_norm)\n",
    "            self.actor_optim.step()\n",
    "            self.actor_scheduler.step(epoch=current_timestep)\n",
    "\n",
    "            # Critics soft update\n",
    "            self.soft_update(self.target_critic, self.critic)\n",
    "            self.soft_update(self.target_critic2, self.critic2)\n",
    "\n",
    "            # Actor soft update\n",
    "            self.soft_update(self.target_actor, self.actor)\n",
    "\n",
    "        return critics_loss.item(), actor_loss.item()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Запуск обучения агента\n",
    "Не забудьте потьюнить гиперпараметры для лучшего результата!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.13.3"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>C:\\Users\\uvd20\\PycharmProjects\\SpectralTechnologies_Autumn_2022_test_task\\wandb\\run-20220930_130625-2ly2ibs6</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href=\"https://wandb.ai/uvd174/DRL_LunarLanderContinuous-v2/runs/2ly2ibs6\" target=\"_blank\">prime-planet-17</a></strong> to <a href=\"https://wandb.ai/uvd174/DRL_LunarLanderContinuous-v2\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()\n",
    "wandb.init(project='DRL_LunarLanderContinuous-v2')\n",
    "wandb.config = {\n",
    "    'model_name': 'DDPG',\n",
    "    'lr': 0.00025,\n",
    "    'timesteps': 1_000_000,\n",
    "    'batch_size': 64,\n",
    "    'hidden_size': 256,\n",
    "    'tau': 0.005,\n",
    "    'gamma': 0.99,\n",
    "    'start_train': 10000,\n",
    "    'adam_eps': 1e-7,\n",
    "    'action_noise': 0.25,\n",
    "    'max_grad_norm': 0.5,\n",
    "}\n",
    "config = wandb.config\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "qsS_eniivAvX"
   },
   "outputs": [],
   "source": [
    "xs, means, stds, labels = [], [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "E7VgD8zVYu3u"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/500000 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "08469ea0599141f4ae17c37dfeb7ec49"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent mean score is 260.16829010372624 with std 53.076225486966656\n"
     ]
    }
   ],
   "source": [
    "# Train DDPG model\n",
    "ddpg = DDPG(\n",
    "    STATE_SIZE, ACTION_SIZE, tau=config['tau'], gamma=config['gamma'], lr=config['lr'],\n",
    "    adam_eps=config['adam_eps'], hidden_size=config['hidden_size'], timesteps=config['timesteps'],\n",
    "    max_grad_norm=config['max_grad_norm'],\n",
    ")\n",
    "x, mean, std = train(\n",
    "    ddpg, timesteps=config['timesteps'], test_every=10000, batch_size=config['batch_size'],\n",
    "    start_train=config['start_train'], buffer_size=50000, test_count=50, logger=wandb,\n",
    "    action_noise=config['action_noise'],\n",
    ")\n",
    "\n",
    "xs.append(x)\n",
    "means.append(mean)\n",
    "stds.append(std)\n",
    "labels.append('DDPG')\n",
    "\n",
    "final_mean, final_std, _ = test(ddpg, episodes=100)\n",
    "wandb.log({'mean': final_mean, 'std': final_std, 'step': config['timesteps'], 'cur_lr': 0}, step=config['timesteps'])\n",
    "print(f'Agent mean score is {final_mean} with std {final_std}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "save_paths = [\n",
    "    f'saved_models/ddpg_best_weights_actor.pt',\n",
    "    f'saved_models/ddpg_best_weights_critic.pt',\n",
    "    f'saved_models/ddpg_best_weights_target_actor.pt',\n",
    "    f'saved_models/ddpg_best_weights_target_critic.pt',\n",
    "]\n",
    "save_nets = [ddpg.actor, ddpg.critic, ddpg.target_actor, ddpg.target_critic]\n",
    "\n",
    "for path, net in zip(save_paths, save_nets):\n",
    "    net.load_state_dict(torch.load(path))\n",
    "\n",
    "best_mean, best_std, _ = test(ddpg, episodes=1000)\n",
    "print(best_mean, best_std)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "VBox(children=(Label(value='1.057 MB of 2.114 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.500149…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0834dadc8ec84cb89c5e5d4a902daed2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>actor_loss</td><td>█▇▇▇▇▇▇▇▇▆▆▆▆▆▆▆▆▇████▃▂▁▂▄▄▅▆▆▇▇▇▇▇████</td></tr><tr><td>critic_loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▅█▇█▁▁▁▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>cur_lr</td><td>▁</td></tr><tr><td>mean</td><td>▆▇▇█▅█████████▇▇▇▆▇█▁▄▄▃▄▅▅▆▇▄▇█▇▆▇▆▇▆▇█</td></tr><tr><td>std</td><td>▆▅▄▁█▂▁▁▂▃▂▁▁▃▄▄▃▄▅▃▂▃▄▁▅▄▆▄▃▇▅▄▅▇▅▆▃▆▄▂</td></tr><tr><td>step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>actor_loss</td><td>-86.0911</td></tr><tr><td>critic_loss</td><td>202.9585</td></tr><tr><td>cur_lr</td><td>0</td></tr><tr><td>mean</td><td>260.16829</td></tr><tr><td>std</td><td>53.07623</td></tr><tr><td>step</td><td>500000</td></tr></table><br/></div></div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Synced <strong style=\"color:#cdcd00\">prime-planet-17</strong>: <a href=\"https://wandb.ai/uvd174/DRL_LunarLanderContinuous-v2/runs/2ly2ibs6\" target=\"_blank\">https://wandb.ai/uvd174/DRL_LunarLanderContinuous-v2/runs/2ly2ibs6</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 8 other file(s)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find logs at: <code>.\\wandb\\run-20220930_130625-2ly2ibs6\\logs</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.13.3"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>C:\\Users\\uvd20\\PycharmProjects\\SpectralTechnologies_Autumn_2022_test_task\\wandb\\run-20220930_223441-3fht8zej</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href=\"https://wandb.ai/uvd174/DRL_LunarLanderContinuous-v2/runs/3fht8zej\" target=\"_blank\">astral-darkness-26</a></strong> to <a href=\"https://wandb.ai/uvd174/DRL_LunarLanderContinuous-v2\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()\n",
    "wandb.init(project='DRL_LunarLanderContinuous-v2')\n",
    "wandb.config = {\n",
    "    'model_name': 'TD3',\n",
    "    'lr': 0.00025,\n",
    "    'timesteps': 1_000_000,\n",
    "    'batch_size': 64,\n",
    "    'hidden_size': 256,\n",
    "    'tau': 0.005,\n",
    "    'gamma': 0.99,\n",
    "    'start_train': 20000,\n",
    "    'adam_eps': 1e-7,\n",
    "    'action_noise': 0.25,\n",
    "    'max_grad_norm': 0.5,\n",
    "    'policy_freq': 3,\n",
    "}\n",
    "config = wandb.config"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "Dg-7X7Z2vBMn"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/1000000 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1ec093c5e23245919f45036ff3ff9306"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'xs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [53], line 13\u001B[0m\n\u001B[0;32m      2\u001B[0m td3 \u001B[38;5;241m=\u001B[39m TD3(\n\u001B[0;32m      3\u001B[0m     STATE_SIZE, ACTION_SIZE, tau\u001B[38;5;241m=\u001B[39mconfig[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtau\u001B[39m\u001B[38;5;124m'\u001B[39m], gamma\u001B[38;5;241m=\u001B[39mconfig[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mgamma\u001B[39m\u001B[38;5;124m'\u001B[39m], lr\u001B[38;5;241m=\u001B[39mconfig[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlr\u001B[39m\u001B[38;5;124m'\u001B[39m],\n\u001B[0;32m      4\u001B[0m     adam_eps\u001B[38;5;241m=\u001B[39mconfig[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124madam_eps\u001B[39m\u001B[38;5;124m'\u001B[39m], hidden_size\u001B[38;5;241m=\u001B[39mconfig[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhidden_size\u001B[39m\u001B[38;5;124m'\u001B[39m], timesteps\u001B[38;5;241m=\u001B[39mconfig[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtimesteps\u001B[39m\u001B[38;5;124m'\u001B[39m],\n\u001B[0;32m      5\u001B[0m     max_grad_norm\u001B[38;5;241m=\u001B[39mconfig[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmax_grad_norm\u001B[39m\u001B[38;5;124m'\u001B[39m], policy_freq\u001B[38;5;241m=\u001B[39mconfig[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpolicy_freq\u001B[39m\u001B[38;5;124m'\u001B[39m],\n\u001B[0;32m      6\u001B[0m )\n\u001B[0;32m      7\u001B[0m x, mean, std \u001B[38;5;241m=\u001B[39m train(\n\u001B[0;32m      8\u001B[0m     td3, timesteps\u001B[38;5;241m=\u001B[39mconfig[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtimesteps\u001B[39m\u001B[38;5;124m'\u001B[39m], test_every\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m20000\u001B[39m, batch_size\u001B[38;5;241m=\u001B[39mconfig[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbatch_size\u001B[39m\u001B[38;5;124m'\u001B[39m],\n\u001B[0;32m      9\u001B[0m     start_train\u001B[38;5;241m=\u001B[39mconfig[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mstart_train\u001B[39m\u001B[38;5;124m'\u001B[39m], buffer_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m50000\u001B[39m, test_count\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m, logger\u001B[38;5;241m=\u001B[39mwandb,\n\u001B[0;32m     10\u001B[0m     action_noise\u001B[38;5;241m=\u001B[39mconfig[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124maction_noise\u001B[39m\u001B[38;5;124m'\u001B[39m],\n\u001B[0;32m     11\u001B[0m )\n\u001B[1;32m---> 13\u001B[0m xs\u001B[38;5;241m.\u001B[39mappend(x)\n\u001B[0;32m     14\u001B[0m means\u001B[38;5;241m.\u001B[39mappend(mean)\n\u001B[0;32m     15\u001B[0m stds\u001B[38;5;241m.\u001B[39mappend(std)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'xs' is not defined"
     ]
    }
   ],
   "source": [
    "# Train TD3 model\n",
    "td3 = TD3(\n",
    "    STATE_SIZE, ACTION_SIZE, tau=config['tau'], gamma=config['gamma'], lr=config['lr'],\n",
    "    adam_eps=config['adam_eps'], hidden_size=config['hidden_size'], timesteps=config['timesteps'],\n",
    "    max_grad_norm=config['max_grad_norm'], policy_freq=config['policy_freq'],\n",
    ")\n",
    "x, mean, std = train(\n",
    "    td3, timesteps=config['timesteps'], test_every=20000, batch_size=config['batch_size'],\n",
    "    start_train=config['start_train'], buffer_size=50000, test_count=10, logger=wandb,\n",
    "    action_noise=config['action_noise'],\n",
    ")\n",
    "\n",
    "xs.append(x)\n",
    "means.append(mean)\n",
    "stds.append(std)\n",
    "labels.append('TD3')\n",
    "\n",
    "final_mean, final_std, _ = test(td3, episodes=100)\n",
    "print(f'Agent mean score is {final_mean} with std {final_std}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a89b7aa820fb4735a4036123256029fc"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>actor_loss</td><td>  ▁  ███████████████████████████████████</td></tr><tr><td>critic_loss</td><td>▃█▅▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>mean</td><td>▇▆▅▆▁▄▆▆▇▇█▆▅▅▆▆▆▇▆▇█▇▇▇▇▇█▆█▇█▇▇▇▆██▇▇█</td></tr><tr><td>std</td><td>▁▁▁▁█▂▂▁▁▂▁▂▂▂▂▂▃▂▂▂▁▂▂▃▂▂▂▃▁▂▁▂▂▂▃▂▁▂▃▂</td></tr><tr><td>step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>actor_loss</td><td>2.6285726107095065e+32</td></tr><tr><td>critic_loss</td><td>73.76964</td></tr><tr><td>mean</td><td>-36.43455</td></tr><tr><td>std</td><td>40.33247</td></tr><tr><td>step</td><td>999999</td></tr></table><br/></div></div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Synced <strong style=\"color:#cdcd00\">astral-darkness-26</strong>: <a href=\"https://wandb.ai/uvd174/DRL_LunarLanderContinuous-v2/runs/3fht8zej\" target=\"_blank\">https://wandb.ai/uvd174/DRL_LunarLanderContinuous-v2/runs/3fht8zej</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find logs at: <code>.\\wandb\\run-20220930_223441-3fht8zej\\logs</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "build_plot(xs, means, stds, labels)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "263.48935154371196 45.08315451583006\n"
     ]
    }
   ],
   "source": [
    "# Show the best DDPG results\n",
    "ddpg = DDPG(STATE_SIZE, ACTION_SIZE)\n",
    "\n",
    "load_paths = [\n",
    "    f'saved_models/ddpg_best_weights_actor.pt',\n",
    "    f'saved_models/ddpg_best_weights_critic.pt',\n",
    "    f'saved_models/ddpg_best_weights_target_actor.pt',\n",
    "    f'saved_models/ddpg_best_weights_target_critic.pt',\n",
    "]\n",
    "load_nets = [\n",
    "    ddpg.actor, ddpg.critic,\n",
    "    ddpg.target_actor, ddpg.target_critic,\n",
    "]\n",
    "\n",
    "for path, net in zip(load_paths, load_nets):\n",
    "    net.load_state_dict(torch.load(path))\n",
    "\n",
    "best_mean, best_std, _ = test(ddpg, episodes=100)\n",
    "print(best_mean, best_std)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-95.26885674598375 65.3958612866716\n"
     ]
    }
   ],
   "source": [
    "# Show the best TD3 results\n",
    "td3 = TD3(STATE_SIZE, ACTION_SIZE)\n",
    "\n",
    "load_paths = [\n",
    "    f'saved_models/td3_best_weights_actor.pt',\n",
    "    f'saved_models/td3_best_weights_critic.pt',\n",
    "    f'saved_models/td3_best_weights_critic2.pt',\n",
    "    f'saved_models/td3_best_weights_target_actor.pt',\n",
    "    f'saved_models/td3_best_weights_target_critic.pt',\n",
    "    f'saved_models/td3_best_weights_target_critic2.pt',\n",
    "]\n",
    "load_nets = [\n",
    "    td3.actor, td3.critic, td3.critic2,\n",
    "    td3.target_actor, td3.target_critic, td3.target_critic2,\n",
    "]\n",
    "\n",
    "for path, net in zip(load_paths, load_nets):\n",
    "    net.load_state_dict(torch.load(path))\n",
    "\n",
    "best_mean, best_std, _ = test(td3, episodes=100)\n",
    "print(best_mean, best_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}